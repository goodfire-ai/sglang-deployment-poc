# SGLang Deployment Environment Configuration
# Copy this file to .env and fill in your values

# ===========================================
# HuggingFace Configuration (REQUIRED)
# ===========================================
# Get your token from: https://huggingface.co/settings/tokens
# Required for downloading gated models like Llama
HF_TOKEN=your_huggingface_token_here

# HuggingFace cache directory (optional, defaults to ~/.cache/huggingface)
# HF_HOME=/path/to/cache

# ===========================================
# Model Configuration
# ===========================================
# Model to deploy (HuggingFace path or local directory)
MODEL_PATH=meta-llama/Meta-Llama-3-70B-Instruct

# Alternative models:
# MODEL_PATH=meta-llama/Llama-3.1-70B-Instruct
# MODEL_PATH=meta-llama/Llama-3.1-405B-Instruct
# MODEL_PATH=/path/to/local/model

# ===========================================
# Server Configuration
# ===========================================
# Server host (0.0.0.0 for external access, 127.0.0.1 for local only)
SERVER_HOST=0.0.0.0

# Server port
SERVER_PORT=30000

# ===========================================
# GPU Configuration
# ===========================================
# Tensor parallelism size (number of GPUs to split model across)
TENSOR_PARALLEL_SIZE=4

# Data parallelism size (optional, for throughput scaling)
# DATA_PARALLEL_SIZE=1

# Pipeline parallelism size (optional)
# PIPELINE_PARALLEL_SIZE=1

# Memory fraction for KV cache (0.0-1.0, default 0.9)
# Lower this if you get OOM errors
MEM_FRACTION=0.85

# ===========================================
# Advanced Configuration (Optional)
# ===========================================
# Quantization method (fp8, awq, gptq, bitsandbytes)
# QUANTIZATION=fp8

# Attention backend (flashinfer, triton, fa3, fa4)
# ATTENTION_BACKEND=flashinfer

# Data type (auto, half, bfloat16, float32)
# DTYPE=auto

# ===========================================
# Slurm Configuration (for cluster deployment)
# ===========================================
# Project directory on cluster
PROJECT_DIR=$HOME/sglang-deployment-poc

# Slurm partition name
SLURM_PARTITION=gpu

# GPU type to request (h100, a100, etc.)
SLURM_GPU_TYPE=h100

# Number of GPUs per node
SLURM_GPUS_PER_NODE=4

# Job time limit (HH:MM:SS)
SLURM_TIME_LIMIT=24:00:00

# Memory per job
SLURM_MEMORY=200G

# CUDA module to load
CUDA_MODULE=cuda/12.1

# Python module to load
PYTHON_MODULE=python/3.10
